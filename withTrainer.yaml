# Boot order:
# sparkTrainer => elasticsearch,zookeeper,daemon,kafkaserver => logstash,kafkaUI,sparkExecutor,kibana
  
  version: '3.7'

  services:

    #--- Kafka ---
    zookeeper:
        image: tap:kafka
        container_name: kafkaZK
        environment:
            - KAFKA_ACTION=start-zk
        networks: 
            tap:
                ipv4_address: 10.0.100.22
        depends_on:
         sparkTrainer:
            condition: service_completed_successfully

    kafkaserver:
        image: tap:kafka
        container_name: kafkaServer
        environment:
            - KAFKA_ACTION=start-kafka
            #- KAFKA_HEAP_OPTS=-Xmx256M
        ports:
            - 9092:9092
        networks: 
            tap:
                ipv4_address: 10.0.100.23
        healthcheck:
          test: curl -f http://elasticsearch:9200 || exit 1
          interval: 60s
          retries: 10
          start_period: 20s
        depends_on:
          sparkTrainer:
              condition: service_completed_successfully

    kafka-ui:
        image: provectuslabs/kafka-ui:latest
        container_name: kafkaWebUI
        environment:
            - KAFKA_CLUSTERS_0_NAME=local
            - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafkaServer:9092
        ports: 
            - 8080:8080
        networks: 
            - tap
        depends_on:
            kafkaserver:
              condition: service_healthy


    #--- Logstash ---
    logstash:
      image: tap:logstash   
      networks: 
        - tap
      environment:
        - XPACK_MONITORING_ENABLED=false
        - KAFKA_OUTPUT_BOOTSTRAP_SERVERS=kafkaserver:9092
        - KAFKA_OUTPUT_TOPIC=articles
      volumes:
        - ./logstash/pipeline/progetto.conf:/usr/share/logstash/pipeline/logstash.conf
        - dataframe:/usr/share/logstash/dataframe
      depends_on:
        kafkaserver:
          condition: service_healthy

    #--- Daemon ---
    daemon:
      image: tap:daemon
      environment:
        - PYTHONUNBUFFERED=1 # To let python print() be displayed. Comment to mute daemon
      networks:
        - tap
      volumes:
        - dataframe:/app/dataframe
      depends_on:
         sparkTrainer:
            condition: service_completed_successfully

    #--- Spark ---  
    sparkTrainer:
      image: tap:sparkP
      container_name: sparkTrainer
      environment:
        - SPARK_ACTION=bash
      volumes:
        - training:/opt/tap/training
        - sparklibs:/root/.ivy2
      deploy:
        resources:
          limits:
            cpus: '4.0'
            memory: 4g
      networks:
        - tap
      ports:
        - 4040:4040
      command: spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.elasticsearch:elasticsearch-spark-30_2.12:8.2.0 --master local[*] /opt/tap/trainer17cls.py
      # to run an interactive container for exploring the machine:
      # docker run --entrypoint /bin/bash -v training:/opt/tap/training -it tap:sparkP

    sparkExecutor:
      image: tap:sparkP
      container_name: sparkExecutor
      environment:
        - SPARK_ACTION=bash
      volumes:
        - training:/opt/tap/training
        - sparklibs:/root/.ivy2
      deploy:
        resources:
          limits:
            cpus: '4.0'
            memory: 4g
      networks:
        - tap
      ports:
        - 4040:4040
      command: spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2,org.elasticsearch:elasticsearch-spark-30_2.12:8.2.0 --master local[*] /opt/tap/executor.py
      depends_on:
        kafkaserver:
          condition: service_healthy

    #--- ElasticSearch ---
    elasticsearch:
      container_name: elasticsearch
      hostname: elasticsearch
      image: docker.elastic.co/elasticsearch/elasticsearch:8.2.0
      ports:
        - "9200:9200"
        - "9300:9300"      
      environment:
        - node.name=elasticsearch
        - xpack.security.enabled=false
        - discovery.type=single-node
        - bootstrap.memory_lock=true
        - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
        - cluster.routing.allocation.disk.threshold_enabled=false
      ulimits:
        memlock:
          soft: -1
          hard: -1
      networks: 
        - tap
      depends_on:
         sparkTrainer:
            condition: service_completed_successfully

    #--- Kibana ---
    kibana:
      container_name: kibana
      hostname: kibana
      image: docker.elastic.co/kibana/kibana:8.2.0
      build: 
        context: kibana
        dockerfile: Dockerfile
      networks: 
        - tap
      ports:
        - 5601:5601
      environment:
        - xpack.security.enabled=false
      depends_on:
        kafkaserver:
          condition: service_healthy

  volumes:
    dataframe:
    training:
    sparklibs:

  networks:
    tap:
        name: tap
        driver: bridge
        ipam:
            config:
                - subnet: 10.0.100.1/24